{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dee336a-1518-4e32-9406-9771688b610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (1.33.1)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Obtaining dependency information for google-cloud-aiplatform from https://files.pythonhosted.org/packages/b4/ab/339400d48a1751f689d13e68ecf21428176eb41a821032d47eaa312f404c/google_cloud_aiplatform-1.36.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_aiplatform-1.36.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: google-cloud-storage in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (1.42.0)\n",
      "Collecting google-cloud-storage\n",
      "  Obtaining dependency information for google-cloud-storage from https://files.pythonhosted.org/packages/04/72/71b1b531cefa1daff8f6a2a70b4d4fa18dd4da851b5486d53578811b0838/google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery[pandas] in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (3.12.0)\n",
      "Collecting google-cloud-bigquery[pandas]\n",
      "  Obtaining dependency information for google-cloud-bigquery[pandas] from https://files.pythonhosted.org/packages/51/8c/bf168c5450431734d67ed4db3e62e2c81fbf2c7d8c0ff3153808e9ab480f/google_cloud_bigquery-3.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_bigquery-3.13.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tqdm in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.12.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-aiplatform) (4.25.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.10.4)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-storage) (2.23.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-bigquery[pandas]) (1.59.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-bigquery[pandas]) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-bigquery[pandas]) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-bigquery[pandas]) (14.0.0)\n",
      "Requirement already satisfied: db-dtypes<2.0.0dev,>=0.3.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-bigquery[pandas]) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from db-dtypes<2.0.0dev,>=0.3.0->google-cloud-bigquery[pandas]) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.61.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from pandas>=1.1.0->google-cloud-bigquery[pandas]) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery[pandas]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/vkoova/.local/share/virtualenvs/notebooks-JGCxN3sD/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.5.0)\n",
      "Using cached google_cloud_aiplatform-1.36.0-py2.py3-none-any.whl (3.1 MB)\n",
      "Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_bigquery-3.13.0-py2.py3-none-any.whl (222 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: google-cloud-storage, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 1.42.0\n",
      "    Uninstalling google-cloud-storage-1.42.0:\n",
      "      Successfully uninstalled google-cloud-storage-1.42.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.12.0\n",
      "    Uninstalling google-cloud-bigquery-3.12.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.12.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.33.1\n",
      "    Uninstalling google-cloud-aiplatform-1.33.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.33.1\n",
      "Successfully installed google-cloud-aiplatform-1.36.0 google-cloud-bigquery-3.13.0 google-cloud-storage-2.13.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]' \\\n",
    "                        tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2632d1cb-589f-405e-9985-c852c72a71c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 10:07:25.806174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Any, Generator\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from typing import List, Optional\n",
    "\n",
    "# Load the \"Vertex AI Embeddings for Text\" model\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "PROJECT_ID = \"viki-dev-app-wsky\"\n",
    "REGION = \"us-east4\"\n",
    "BUCKET_URI = \"gs://viki-ai-provisional-dev/icd10-search-balki/\"\n",
    "# The number of dimensions for the tensorflow universal sentence encoder.\n",
    "# If other embedder is used, the dimensions would probably need to change.\n",
    "DIMENSIONS = 512\n",
    "DISPLAY_NAME = \"balki-icd10-index\"\n",
    "DESCRIPTION = \"icd10-index\"\n",
    "EMBEDDING_DIR = BUCKET_URI + \"/\" + \"icd10-index\"\n",
    "DEPLOYED_INDEX_ID = \"balki-icd10-index\"\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8ad695-4bdb-4cdb-8bc1-48de6e8e3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "\n",
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: int = 10, batch_size: int = 5\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful\n",
    "\n",
    "# Define an embedding method that uses the model\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]\n",
    "\n",
    "def query_bigquery_chunks(\n",
    "    max_rows: int, rows_per_chunk: int, start_chunk: int = 0\n",
    ") -> Generator[pd.DataFrame, Any, None]:\n",
    "    for offset in range(start_chunk, max_rows, rows_per_chunk):\n",
    "        query = QUERY_TEMPLATE.format(limit=rows_per_chunk, offset=offset)\n",
    "        query_job = client.query(query)\n",
    "        rows = query_job.result()\n",
    "        df = rows.to_dataframe()\n",
    "        df[\"code_with_desc\"] = df.code + \"\\n\" + df.desc\n",
    "        yield df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4696e6cc-03f1-4e04-bb16-25f61f6fc139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c983f7adfa44a0a156cf113e25e8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "Query code = A0472\n",
      "  Enterocolitis due to Clostridium difficile, not specified as recurrent\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "        SELECT distinct q.code, q.desc\n",
    "        FROM (SELECT * FROM `viki-dev-app-wsky.genai.icd10_codes`) AS q \n",
    "        LIMIT 200 OFFSET 0;\n",
    "        \"\"\"\n",
    "df = next(query_bigquery_chunks(max_rows=1000, rows_per_chunk=1000))\n",
    "\n",
    "# Examine the data\n",
    "df.head()\n",
    "\n",
    "# Encode a subset of questions for validation\n",
    "codes = df.code_with_desc.tolist()[:500]\n",
    "is_successful, code_embeddings = encode_text_to_embedding_batched(\n",
    "    sentences=df.code.tolist()[:500]\n",
    ")\n",
    "\n",
    "# Filter for successfully embedded sentences\n",
    "codes = np.array(codes)[is_successful]\n",
    "\n",
    "DIMENSIONS = len(code_embeddings[0])\n",
    "\n",
    "print(DIMENSIONS)\n",
    "\n",
    "code_index = random.randint(0, 99)\n",
    "\n",
    "print(f\"Query code = {codes[code_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5e6b19-cc3c-4054-95eb-753b263a8369",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get similarity scores for each embedding by using dot-product.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#scores = np.dot(code_embeddings[code_index], code_embeddings.T)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m encode_texts_to_embeddings(sentences\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintestinal infections\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print top 20 matches\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, (code, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(codes, scores), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m20\u001b[39m]\n\u001b[1;32m      9\u001b[0m ):\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "# Get similarity scores for each embedding by using dot-product.\n",
    "#scores = np.dot(code_embeddings[code_index], code_embeddings.T)\n",
    "query_embedding = encode_texts_to_embeddings(sentences=[\"intestinal infections\"])\n",
    "scores = np.dot(query_embedding[0], code_embeddings.T)\n",
    "\n",
    "# Print top 20 matches\n",
    "for index, (code, score) in enumerate(\n",
    "    sorted(zip(codes, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "):\n",
    "    print(f\"\\t{index}: {code}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876c1b6e-0414-4a7f-ac5c-913372c8277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings directory: /var/folders/v9/f8ypf65s18v2n9q58yfvl20h0000gq/T/tmp8h7ofuuj\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary file to write embeddings to\n",
    "embeddings_file_path = Path(tempfile.mkdtemp())\n",
    "\n",
    "print(f\"Embeddings directory: {embeddings_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a679c0-e68c-4037-aedb-beef60536180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5da888fd1314d99bf27c8e488471fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk of rows from BigQuery:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61872aa18773443faded6fa146176b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "BQ_NUM_ROWS = 10\n",
    "BQ_CHUNK_SIZE = 1000\n",
    "BQ_NUM_CHUNKS = math.ceil(BQ_NUM_ROWS / BQ_CHUNK_SIZE)\n",
    "\n",
    "START_CHUNK = 0\n",
    "\n",
    "# Create a rate limit of 300 requests per minute. Adjust this depending on your quota.\n",
    "API_CALLS_PER_SECOND = 300 / 60\n",
    "# According to the docs, each request can process 5 instances per request\n",
    "ITEMS_PER_REQUEST = 5\n",
    "\n",
    "# Loop through each generated dataframe, convert\n",
    "for i, df in tqdm(\n",
    "    enumerate(\n",
    "        query_bigquery_chunks(\n",
    "            max_rows=BQ_NUM_ROWS, rows_per_chunk=BQ_CHUNK_SIZE, start_chunk=START_CHUNK\n",
    "        )\n",
    "    ),\n",
    "    total=BQ_NUM_CHUNKS - START_CHUNK,\n",
    "    position=-1,\n",
    "    desc=\"Chunk of rows from BigQuery\",\n",
    "):\n",
    "    # Create a unique output file for each chunk\n",
    "    chunk_path = embeddings_file_path.joinpath(\n",
    "        f\"{embeddings_file_path.stem}_{i+START_CHUNK}.json\"\n",
    "    )\n",
    "    with open(chunk_path, \"a\") as f:\n",
    "        id_chunk = df.code\n",
    "\n",
    "        # Convert batch to embeddings\n",
    "        is_successful, question_chunk_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences=df.code_with_desc,\n",
    "            api_calls_per_second=API_CALLS_PER_SECOND,\n",
    "            batch_size=ITEMS_PER_REQUEST,\n",
    "        )\n",
    "\n",
    "        # Append to file\n",
    "        embeddings_formatted = [\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": str(id),\n",
    "                    \"embedding\": [str(value) for value in embedding],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "            for id, embedding in zip(id_chunk[is_successful], question_chunk_embeddings)\n",
    "        ]\n",
    "        f.writelines(embeddings_formatted)\n",
    "\n",
    "        # Delete the DataFrame and any other large data structures\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80929368-b9ec-46e0-a8ec-0fe5fc8f237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=1rh803CZJirAbn6wtm0ntFhVbQGM4I&access_type=offline&code_challenge=7VEZeVztwgp_TFDyYJ1xtF-WoI74QsCQGVQ4Wbk-Xm4&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [Balki.Nakshatrala@wellsky.com].\n",
      "Your current project is [viki-dev-app-wsky].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "gs://viki-ai-provisional-dev/icd10-search-balki//tmp8h7ofuuj/\n",
      "/var/folders/v9/f8ypf65s18v2n9q58yfvl20h0000gq/T/tmp8h7ofuuj\n",
      "If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o \"GSUtil:parallel_process_count=1\"`. Note that multithreading is still available even if you disable multiprocessing.\n",
      "\n",
      "Copying file:///var/folders/v9/f8ypf65s18v2n9q58yfvl20h0000gq/T/tmp8h7ofuuj/tmp8h7ofuuj_0.json [Content-Type=application/json]...\n",
      "- [1/1 files][  3.5 MiB/  3.5 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/3.5 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gcloud auth login\n",
    "#! gcloud auth application-default login\n",
    "remote_folder = f\"{BUCKET_URI}/{embeddings_file_path.stem}/\"\n",
    "print(remote_folder)\n",
    "print(embeddings_file_path)\n",
    "! gsutil -m cp -r {embeddings_file_path}/* {remote_folder} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135f2372-3754-400e-8874-7843e14d22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76550e73-ae5b-41e6-9e8c-95884dcda7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/145042810266/locations/us-east4/indexes/7916290205940711424/operations/989297681719361536\n"
     ]
    }
   ],
   "source": [
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=remote_folder,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=80,\n",
    "    description=DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86ff65-b307-46d0-84b4-eb32c5b4472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a45f2-0ec9-4cc2-a0c6-7ef84dfa67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b74b782-db57-4d2d-a774-44f2835c19ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/145042810266/locations/us-east4/indexEndpoints/8048126048157564928/operations/8633243255506468864\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/145042810266/locations/us-east4/indexEndpoints/8048126048157564928\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/145042810266/locations/us-east4/indexEndpoints/8048126048157564928')\n"
     ]
    }
   ],
   "source": [
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5f461-762a-4fc0-9a7e-e9390dac6b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6e09f2-ac6c-418e-bafc-307e257337a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting db-dtypes\n",
      "  Downloading db_dtypes-1.1.1-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/balki.nakshatrala/.pyenv/versions/3.11.2/envs/viki/lib/python3.11/site-packages (from db-dtypes) (23.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/balki.nakshatrala/.pyenv/versions/3.11.2/envs/viki/lib/python3.11/site-packages (from db-dtypes) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in /Users/balki.nakshatrala/.pyenv/versions/3.11.2/envs/viki/lib/python3.11/site-packages (from db-dtypes) (14.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/balki.nakshatrala/.pyenv/versions/3.11.2/envs/viki/lib/python3.11/site-packages (from db-dtypes) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/balki.nakshatrala/.pyenv/versions/3.11.2/envs/viki/lib/python3.11/site-packages (from pandas>=0.24.2->db-dtypes) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/balki.nakshatrala/.pyenv/versions/3.11.2/envs/viki/lib/python3.11/site-packages (from pandas>=0.24.2->db-dtypes) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/balki.nakshatrala/.pyenv/versions/3.11.2/envs/viki/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->db-dtypes) (1.16.0)\n",
      "Installing collected packages: db-dtypes\n",
      "Successfully installed db-dtypes-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a1b56d-07b4-4df8-99ba-369f86ea817b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform_v1\n",
    "from google.cloud import aiplatform\n",
    "REGION = \"us-east4\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "tree_ah_index1  = aiplatform.MatchingEngineIndex(\n",
    "    index_name='2569954498298511360'\n",
    ")\n",
    "INDEX_RESOURCE_NAME = tree_ah_index1.resource_name\n",
    "\n",
    "index_client = aiplatform_v1.IndexServiceClient(\n",
    "    client_options=dict(api_endpoint=ENDPOINT)\n",
    ")\n",
    "\n",
    "embed = encode_texts_to_embeddings([\"hello world\"])\n",
    "\n",
    "insert_datapoints_payload = aiplatform_v1.IndexDatapoint(\n",
    "    datapoint_id=\"101\",\n",
    "    feature_vector=embed[0],\n",
    "    restricts=[{\"namespace\": \"class\", \"allow_list\": [\"101\"]}],\n",
    "    crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"b\"),\n",
    ")\n",
    "\n",
    "upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
    "    index=INDEX_RESOURCE_NAME, datapoints=[insert_datapoints_payload]\n",
    ")\n",
    "\n",
    "index_client.upsert_datapoints(request=upsert_request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f236c45d-ca2d-4979-9851-47783a1f4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud.aiplatform.matching_engine import MatchingEngineIndexEndpoint\n",
    "from google.cloud import aiplatform_v1beta1 as vertexai\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.disable_interactive_logging()\n",
    "\n",
    "\n",
    "# 指定されたパスの画像ファイルを EfficientNetB0 でベクトル化する\n",
    "def file_to_embedding(model: tf.keras.Model, path: str) -> list[float]:\n",
    "    raw = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(raw, channels=3)\n",
    "    prediction = model.predict(np.array([image.numpy()]))\n",
    "    return prediction[0].tolist()\n",
    "\n",
    "\n",
    "class Matcher:\n",
    "    def __init__(self, index_endpoint_name: str, deployed_index_id: str):\n",
    "        self._index_endpoint_name = index_endpoint_name\n",
    "        self._deployed_index_id = deployed_index_id\n",
    "\n",
    "        self._client = vertexai.MatchServiceClient(\n",
    "            client_options={\"api_endpoint\": self._public_endpoint()}\n",
    "        )\n",
    "\n",
    "    # Matching Engine にリクエストして、\n",
    "    # 与えられたエンベディングの近似最近傍探索を行う\n",
    "    def find_neighbors(self, embedding: list[float], neighbor_count: int):\n",
    "        datapoint = vertexai.IndexDatapoint(\n",
    "            datapoint_id=\"dummy-id\",\n",
    "            feature_vector=embedding\n",
    "        )\n",
    "        query = vertexai.FindNeighborsRequest.Query(datapoint=datapoint)\n",
    "        request = vertexai.FindNeighborsRequest(\n",
    "            index_endpoint=self._index_endpoint_name,\n",
    "            deployed_index_id=self._deployed_index_id,\n",
    "            queries=[query],\n",
    "        )\n",
    "\n",
    "        resp = self._client.find_neighbors(request)\n",
    "\n",
    "        return resp.nearest_neighbors[0].neighbors\n",
    "\n",
    "    # IndexEndpoint の public endpoint を取得する\n",
    "    def _public_endpoint(self) -> str:\n",
    "        endpoint = MatchingEngineIndexEndpoint(\n",
    "            index_endpoint_name=self._index_endpoint_name\n",
    "        )\n",
    "        return endpoint.gca_resource.public_endpoint_domain_name\n",
    "\n",
    "\n",
    "def search_for_text(index_endpoint_name: str,\n",
    "                             deployed_index_id: str,\n",
    "                             text: str) -> None:\n",
    "    embedding = encode_texts_to_embeddings([text])\n",
    "\n",
    "    matcher = Matcher(index_endpoint_name, deployed_index_id)\n",
    "    neighbors = matcher.find_neighbors(embedding[0], 10)\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        datapoint_id = neighbor.datapoint.datapoint_id\n",
    "        distance = neighbor.distance\n",
    "        print(f\"{datapoint_id}\\tdistance={distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d812792-0d86-422a-9843-97b60a02aea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\tdistance=0.8356250524520874\n",
      "A320\tdistance=0.603960394859314\n",
      "A073\tdistance=0.6031581163406372\n",
      "A074\tdistance=0.6029283404350281\n",
      "A33\tdistance=0.6026632785797119\n",
      "A217\tdistance=0.5976741313934326\n",
      "A070\tdistance=0.592897891998291\n",
      "A0104\tdistance=0.592700183391571\n",
      "A067\tdistance=0.589692234992981\n",
      "A071\tdistance=0.5884808897972107\n"
     ]
    }
   ],
   "source": [
    "index_endpoint_name = 'projects/145042810266/locations/us-east4/indexEndpoints/7298276710200377344'\n",
    "deployed_index_id = 'balki_search_1699469904710'\n",
    "\n",
    "search_for_text(index_endpoint_name=index_endpoint_name,\n",
    "                         deployed_index_id=deployed_index_id,\n",
    "                         text=\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e07af-2052-4456-a48c-5fc55b718bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
